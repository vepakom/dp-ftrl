{
{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "enhanced-promise",
      "metadata": {
        "id": "enhanced-promise"
      },
      "source": [
        "## Simple NN with no Zs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "other-harbor",
      "metadata": {
        "id": "other-harbor"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VHjKX7m7X186",
      "metadata": {
        "id": "VHjKX7m7X186"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Tuple\n",
        "from torch.utils.data import SequentialSampler, RandomSampler, BatchSampler\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "developing-disposal",
      "metadata": {
        "id": "developing-disposal"
      },
      "outputs": [],
      "source": [
        "#define the network in pytorch (with Z)\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lr_w, batch_size):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim[0])\n",
        "        self.lin2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
        "        self.lin3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
        "\n",
        "        self.apply(self.init_weights_biases)\n",
        "        self.optimizer_w = torch.optim.Adam(self.get_w(), lr=lr_w)\n",
        "\n",
        "    def init_weights_biases(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight, gain=1.0) #modify with custom function later\n",
        "            nn.init.normal_(m.bias, mean=0.0, std=1.0)\n",
        "\n",
        "    def get_w(self):\n",
        "        return [self.input_layer.weight, self.lin2.weight, self.lin3.weight]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = nn.Tanh()(x)\n",
        "        x = self.lin2(x)\n",
        "        x = nn.Tanh()(x)\n",
        "        x = self.lin3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unlikely-capacity",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unlikely-capacity",
        "outputId": "b4d22a3b-2e33-4260-df45-5b8798fe72d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_train/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5821386.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_train/MNIST/raw/train-images-idx3-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 158932.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_train/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1514977.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_train/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 6968006.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_train/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_test/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5457521.68it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_test/MNIST/raw/train-images-idx3-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 160381.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_test/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1513797.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_test/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2602531.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_test/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input_dim = 784\n",
        "\n",
        "#Parameters setup\n",
        "classes = 10\n",
        "lr_w = 0.001\n",
        "batch_size = 50\n",
        "hidden_dim = [100, 100, 10]\n",
        "epochs = 5\n",
        "\n",
        "#Get MNIST data and define transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "trainset = datasets.MNIST('mnist_train', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('mnist_test', download=True, train=False, transform=transform)\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = NeuralNetwork(input_dim, hidden_dim, lr_w, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "damaged-april",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "damaged-april",
        "outputId": "56f1c85f-d867-4afb-c083-570fa1739401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step[100/1200], Loss: 0.3186\n",
            "Epoch [1/5], Step[200/1200], Loss: 0.3665\n",
            "Epoch [1/5], Step[300/1200], Loss: 0.2403\n",
            "Epoch [1/5], Step[400/1200], Loss: 0.2025\n",
            "Epoch [1/5], Step[500/1200], Loss: 0.1417\n",
            "Epoch [1/5], Step[600/1200], Loss: 0.2379\n",
            "Epoch [1/5], Step[700/1200], Loss: 0.5252\n",
            "Epoch [1/5], Step[800/1200], Loss: 0.3303\n",
            "Epoch [1/5], Step[900/1200], Loss: 0.2004\n",
            "Epoch [1/5], Step[1000/1200], Loss: 0.1924\n",
            "Epoch [1/5], Step[1100/1200], Loss: 0.1139\n",
            "Epoch [1/5], Step[1200/1200], Loss: 0.1396\n",
            "Epoch [2/5], Step[100/1200], Loss: 0.1021\n",
            "Epoch [2/5], Step[200/1200], Loss: 0.1466\n",
            "Epoch [2/5], Step[300/1200], Loss: 0.1977\n",
            "Epoch [2/5], Step[400/1200], Loss: 0.1115\n",
            "Epoch [2/5], Step[500/1200], Loss: 0.0734\n",
            "Epoch [2/5], Step[600/1200], Loss: 0.1672\n",
            "Epoch [2/5], Step[700/1200], Loss: 0.1963\n",
            "Epoch [2/5], Step[800/1200], Loss: 0.0704\n",
            "Epoch [2/5], Step[900/1200], Loss: 0.1572\n",
            "Epoch [2/5], Step[1000/1200], Loss: 0.2647\n",
            "Epoch [2/5], Step[1100/1200], Loss: 0.1526\n",
            "Epoch [2/5], Step[1200/1200], Loss: 0.1408\n",
            "Epoch [3/5], Step[100/1200], Loss: 0.1275\n",
            "Epoch [3/5], Step[200/1200], Loss: 0.0568\n",
            "Epoch [3/5], Step[300/1200], Loss: 0.1402\n",
            "Epoch [3/5], Step[400/1200], Loss: 0.2817\n",
            "Epoch [3/5], Step[500/1200], Loss: 0.1849\n",
            "Epoch [3/5], Step[600/1200], Loss: 0.0618\n",
            "Epoch [3/5], Step[700/1200], Loss: 0.0246\n",
            "Epoch [3/5], Step[800/1200], Loss: 0.0806\n",
            "Epoch [3/5], Step[900/1200], Loss: 0.1582\n",
            "Epoch [3/5], Step[1000/1200], Loss: 0.1362\n",
            "Epoch [3/5], Step[1100/1200], Loss: 0.2792\n",
            "Epoch [3/5], Step[1200/1200], Loss: 0.0561\n",
            "Epoch [4/5], Step[100/1200], Loss: 0.2353\n",
            "Epoch [4/5], Step[200/1200], Loss: 0.1349\n",
            "Epoch [4/5], Step[300/1200], Loss: 0.0141\n",
            "Epoch [4/5], Step[400/1200], Loss: 0.1628\n",
            "Epoch [4/5], Step[500/1200], Loss: 0.1178\n",
            "Epoch [4/5], Step[600/1200], Loss: 0.3589\n",
            "Epoch [4/5], Step[700/1200], Loss: 0.1769\n",
            "Epoch [4/5], Step[800/1200], Loss: 0.2301\n",
            "Epoch [4/5], Step[900/1200], Loss: 0.0886\n",
            "Epoch [4/5], Step[1000/1200], Loss: 0.0937\n",
            "Epoch [4/5], Step[1100/1200], Loss: 0.0301\n",
            "Epoch [4/5], Step[1200/1200], Loss: 0.0378\n",
            "Epoch [5/5], Step[100/1200], Loss: 0.0954\n",
            "Epoch [5/5], Step[200/1200], Loss: 0.1536\n",
            "Epoch [5/5], Step[300/1200], Loss: 0.1400\n",
            "Epoch [5/5], Step[400/1200], Loss: 0.1589\n",
            "Epoch [5/5], Step[500/1200], Loss: 0.0364\n",
            "Epoch [5/5], Step[600/1200], Loss: 0.0371\n",
            "Epoch [5/5], Step[700/1200], Loss: 0.0847\n",
            "Epoch [5/5], Step[800/1200], Loss: 0.0996\n",
            "Epoch [5/5], Step[900/1200], Loss: 0.0565\n",
            "Epoch [5/5], Step[1000/1200], Loss: 0.0746\n",
            "Epoch [5/5], Step[1100/1200], Loss: 0.2061\n",
            "Epoch [5/5], Step[1200/1200], Loss: 0.1277\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = 'cpu'\n",
        "n_total_steps = len(trainloader)\n",
        "for epoch in range(epochs):\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # Backward and optimize\n",
        "        model.optimizer_w.zero_grad()\n",
        "        loss.backward()\n",
        "        model.optimizer_w.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "             print (f'Epoch [{epoch+1}/{epochs}], Step[{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "social-crowd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "social-crowd",
        "outputId": "6b1c8dea-928d-433e-f2f6-7a29104b21aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 96.08 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "acc = 100.0 * n_correct / n_samples\n",
        "print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "taken-james",
      "metadata": {
        "id": "taken-james"
      },
"source": [
        "## Simple NN With 1 Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8ArnHYR8YKc8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ArnHYR8YKc8",
        "outputId": "e2bbdb20-d67d-4de3-a4cc-c111ebc44b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opacus\n",
            "  Downloading opacus-1.5.2-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.26.4)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.4.1+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.13.1)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Downloading opacus-1.5.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.9/239.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opacus\n",
            "Successfully installed opacus-1.5.2\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.4.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.5)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.1)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch_lightning)\n",
            "  Downloading torchmetrics-1.4.3-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch_lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (71.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics>=0.7.0->pytorch_lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.14.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.2.0)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.4.3-py3-none-any.whl (869 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.5/869.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lightning-utilities, torchmetrics, pytorch_lightning\n",
            "Successfully installed lightning-utilities-0.11.8 pytorch_lightning-2.4.0 torchmetrics-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install opacus\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "affecting-coordinator",
      "metadata": {
        "id": "affecting-coordinator"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Tuple\n",
        "from torch.utils.data import SequentialSampler, RandomSampler, BatchSampler\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "Nai_kD4JeMhQ",
      "metadata": {
        "id": "Nai_kD4JeMhQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import json\n",
        "\n",
        "# do\n",
        "import math\n",
        "import sys\n",
        "\n",
        "# from pvc import Dataset\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# from stl.lightning.loggers import ManifoldTensorBoardLogger\n",
        "# from stl.lightning.callbacks.all import ModelCheckpoint\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "y_2DMZbNeXRJ",
      "metadata": {
        "id": "y_2DMZbNeXRJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# do\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# from opacus import PrivacyEngine\n",
        "# from opacus.data_loader import DPDataLoader\n",
        "# from opacus.validators.module_validator import ModuleValidator\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "zdwVcbTPeBSr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdwVcbTPeBSr",
        "outputId": "fe32f2eb-593e-4df0-8bef-fcd281f85b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0\n",
            "-3.0\n",
            "[torch.Size([784, 10])]\n",
            "tensor([-0.2900, -0.2900, -0.2900,  ..., -0.2900, -0.2900, -0.2900],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "tensor([[-0.3736,  1.0039, -0.7738,  ..., -2.4283, -0.7305,  1.3539],\n",
            "        [ 0.7507, -1.6747,  0.6183,  ..., -0.6432, -0.5319,  2.3025],\n",
            "        [-1.5368,  1.6979, -0.1733,  ...,  1.5294,  1.0991, -2.6171],\n",
            "        ...,\n",
            "        [ 1.1072, -0.3868, -0.4830,  ...,  0.7731,  0.3098, -0.6427],\n",
            "        [-0.0406, -0.7681,  0.2690,  ...,  0.1640, -0.5205, -0.0883],\n",
            "        [ 0.9366, -0.2925,  0.4925,  ...,  0.0497, -2.1438,  1.3689]],\n",
            "       requires_grad=True) tensor([[-0.3736,  1.0039, -0.7738,  ..., -2.4283, -0.7305,  1.3539],\n",
            "        [ 0.7507, -1.6747,  0.6183,  ..., -0.6432, -0.5319,  2.3025],\n",
            "        [-1.5368,  1.6979, -0.1733,  ...,  1.5294,  1.0991, -2.6171],\n",
            "        ...,\n",
            "        [ 1.1072, -0.3868, -0.4830,  ...,  0.7731,  0.3098, -0.6427],\n",
            "        [-0.0406, -0.7681,  0.2690,  ...,  0.1640, -0.5205, -0.0883],\n",
            "        [ 0.9366, -0.2925,  0.4925,  ...,  0.0497, -2.1438,  1.3689]],\n",
            "       requires_grad=True)\n",
            "tensor([[ 0.4442,  0.8941, -0.8697,  ..., -1.5947, -0.2858, -1.0052],\n",
            "        [ 0.2770,  1.2464,  0.9731,  ..., -1.9472, -0.9363,  2.5750],\n",
            "        [-0.2827,  1.0896,  0.3533,  ...,  0.0231,  0.6528,  1.6721],\n",
            "        ...,\n",
            "        [-0.6540,  0.5567, -0.5173,  ..., -0.4988, -0.7453,  0.6768],\n",
            "        [-0.7087, -0.5051, -0.8918,  ...,  0.2040,  1.6878, -0.9461],\n",
            "        [ 0.4591,  0.3616, -0.5009,  ..., -0.6667,  0.5156,  0.2178]],\n",
            "       requires_grad=True) tensor([[ 0.4442,  0.8941, -0.8697,  ..., -1.5947, -0.2858, -1.0052],\n",
            "        [ 0.2770,  1.2464,  0.9731,  ..., -1.9472, -0.9363,  2.5750],\n",
            "        [-0.2827,  1.0896,  0.3533,  ...,  0.0231,  0.6528,  1.6721],\n",
            "        ...,\n",
            "        [-0.6540,  0.5567, -0.5173,  ..., -0.4988, -0.7453,  0.6768],\n",
            "        [-0.7087, -0.5051, -0.8918,  ...,  0.2040,  1.6878, -0.9461],\n",
            "        [ 0.4591,  0.3616, -0.5009,  ..., -0.6667,  0.5156,  0.2178]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# TODO: clip norm\n",
        "\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, depth: int, value: float, efficient: bool):\n",
        "        \"\"\"\n",
        "        implements the nodes of the tree. if the tree is efficient we will weigh\n",
        "        the nodes by the weight method. adapted from opacus privacy engine.\n",
        "        \"\"\"\n",
        "        self.depth = depth\n",
        "        self.value = value\n",
        "        self.efficient = efficient\n",
        "\n",
        "    def update_value(self, indx, new_value):\n",
        "        self.value[indx] += new_value\n",
        "\n",
        "    def get_value(self, indx):\n",
        "        \"\"\"as indicated by the efficient implementation of tree aggregation\n",
        "        we reweight it by 1 / (2 - 2^(-depth)) where depth starts from the root.\n",
        "        ex. depth(root) = 0\n",
        "        \"\"\"\n",
        "        if self.efficient:\n",
        "            # print('dd', self.depth)\n",
        "            # print('efficient test', float((1.0 / (2 - math.pow(2, -self.depth))) ** 0.5) * self.value)\n",
        "            return ((1.0 / (2 - math.pow(2, -self.depth))) ** 0.5) * self.value[indx]\n",
        "        else:\n",
        "            return 1.0 * self.value[indx]\n",
        "\n",
        "\n",
        "class TreeAggregation:\n",
        "    \"\"\"\n",
        "    Correlated noise added to gradient sums from a private binary tree\n",
        "    Efficient Implementation Paper (reduces variance in sum of gaussian noise values):\n",
        "        Efficient Use of Differentially Private Binary Trees\n",
        "        https://privacytools.seas.harvard.edu/files/privacytools/files/honaker.pdf\n",
        "    Vanilla Implementation:\n",
        "        Differential Privacy Under Continual Observation\n",
        "        http://www.wisdom.weizmann.ac.il/~naor/PAPERS/continual_observation.pdf\n",
        "\n",
        "    Args:\n",
        "        std = standard deviation of gaussian noise\n",
        "        efficient = decides which kind of tree aggregation to be used\n",
        "        seed = added for reproducibility\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        grad_sizes,\n",
        "        std: float = 1.0,\n",
        "        efficient: bool = False,\n",
        "        seed: int = 0,\n",
        "        mode: str = \"prod\",  # alternatives are prod/test\n",
        "    ):\n",
        "        self.efficient = efficient\n",
        "        self.std = std\n",
        "        self.seed = seed\n",
        "        self.mode = mode\n",
        "        self.grad_sizes = grad_sizes\n",
        "\n",
        "        self.step = 1\n",
        "        self.depth = 0  # TODO: increase depth appropriately\n",
        "        self.max_nodes = 2 ** (self.depth)\n",
        "        self.tree = []\n",
        "\n",
        "        # reproducible experiments\n",
        "        self.generator = torch.Generator()\n",
        "        self.generator.manual_seed(self.seed)\n",
        "\n",
        "    def add_to_tree_helper(\n",
        "        self,\n",
        "        gradient,\n",
        "        noise_to_add,\n",
        "        binary_repr_step,\n",
        "        step,\n",
        "        max_nodes,\n",
        "        add_gradient,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        adds noise to each node on the path from the root node to the leaf node.\n",
        "        Also, returns correlated noise for this specific path to be added to the FTRL optimizer.\n",
        "\n",
        "        Args:\n",
        "            noise_to_add = torch Tensor (gaussian noise) to be added\n",
        "            binary_repr_step = convert current step value to binary representation\n",
        "            step = number of times this function has been called (correlated with maximum nodes in the system)\n",
        "            max_nodes = maximum possible nodes in the tree (i.e. counted after assuming a complete binary tree,\n",
        "                where each node except the leaf node have exactly two childen)\n",
        "\n",
        "        Returns:\n",
        "            correlated noise determined from the current tree structure & step number\n",
        "        \"\"\"\n",
        "        if len(self.tree) == 0:\n",
        "            # print('Root Added')\n",
        "            noise_sum = [torch.zeros(grad_size) for grad_size in self.grad_sizes]\n",
        "            self.tree.append(\n",
        "                TreeNode(depth=self.depth, value=noise_to_add, efficient=self.efficient)\n",
        "            )\n",
        "            # print(len(self.tree), gradient, len(gradient))\n",
        "            if add_gradient:\n",
        "                for indx in range(len(self.grad_sizes)):\n",
        "                    self.tree[0].update_value(indx, gradient[indx])\n",
        "\n",
        "            for indx in range(len(self.grad_sizes)):\n",
        "                noise_sum[indx] += self.tree[0].get_value(indx)\n",
        "            return noise_sum\n",
        "\n",
        "        # node_indx = 0\n",
        "        binary_step_indx = 1\n",
        "\n",
        "        # append noise & init noise_sum\n",
        "        self.tree.append(\n",
        "            TreeNode(depth=self.depth, value=noise_to_add, efficient=self.efficient)\n",
        "        )\n",
        "        noise_sum = [torch.zeros(grad_size) for grad_size in self.grad_sizes]\n",
        "\n",
        "        # for indx in range(len(self.grad_sizes)):\n",
        "        #    self.tree[step].update_value(indx, gradient[indx])\n",
        "\n",
        "        # get back value\n",
        "        if binary_repr_step[0] == \"1\":  # test if left node\n",
        "            for indx in range(len(self.grad_sizes)):\n",
        "                noise_sum[indx] += self.tree[step].get_value(indx)\n",
        "\n",
        "        steps = [step]\n",
        "        while step >= 0 and binary_step_indx < len(binary_repr_step):\n",
        "            left_indx = int((step - 1) / 2)\n",
        "            if step % 2 == 1:  # is left child\n",
        "                step = int((step - 1) / 2)\n",
        "            else:  # is right node\n",
        "                step = int((step - 2) / 2)\n",
        "            steps.append(step)\n",
        "            if add_gradient:\n",
        "                for indx in range(len(self.grad_sizes)):\n",
        "                    self.tree[step].update_value(indx, gradient[indx])\n",
        "\n",
        "            # get value\n",
        "            if binary_repr_step[binary_step_indx] == \"1\" and left_indx < len(self.tree):\n",
        "                for indx in range(len(self.grad_sizes)):\n",
        "                    noise_sum[indx] += self.tree[left_indx].get_value(indx)\n",
        "\n",
        "            binary_step_indx += 1\n",
        "\n",
        "        # print(steps)\n",
        "        return noise_sum\n",
        "\n",
        "    def print_tree(self):\n",
        "        print(\"------------------------Tree Starts------------------------\")\n",
        "        for indx in range(len(self.tree)):\n",
        "            print(\n",
        "                \"(\" + str(indx + 1) + \",\" + str(self.tree[indx].value[0].item()) + \")\",\n",
        "                end=\" \",\n",
        "            )\n",
        "        print(\"\\n------------------------Tree Ends------------------------\")\n",
        "\n",
        "    def add_to_tree_and_get_sum(\n",
        "        self,\n",
        "        gradient: torch.Tensor,\n",
        "        test_noise: Optional[torch.Tensor] = None,\n",
        "        add_gradient: bool = False,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        add the new value to each node along the path\n",
        "\n",
        "        Args:\n",
        "            test_noise = torch.normal(mean=0.0, std=self.std, generator=torch.Generator())\n",
        "\n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        if self.mode == \"prod\":\n",
        "            noise_to_add = []\n",
        "            for indx in range(len(self.grad_sizes)):\n",
        "                # print('size', self.grad_sizes[indx])\n",
        "                noise_to_add.append(\n",
        "                    torch.normal(\n",
        "                        mean=0.0,\n",
        "                        std=self.std,\n",
        "                        size=self.grad_sizes[indx],\n",
        "                        # generator=self.generator\n",
        "                    )\n",
        "                )\n",
        "        elif self.mode == \"test\":\n",
        "            noise_to_add = test_noise\n",
        "\n",
        "        noise_sum = self.add_to_tree_helper(\n",
        "            gradient,\n",
        "            noise_to_add,\n",
        "            np.binary_repr(self.step)[::-1],\n",
        "            self.step - 1,\n",
        "            self.max_nodes + 1,\n",
        "            add_gradient,\n",
        "        )\n",
        "\n",
        "        self.step += 1\n",
        "        # print('metric', self.step, self.max_nodes, self.depth)\n",
        "        if self.step > self.max_nodes:\n",
        "            self.depth += 1\n",
        "            self.max_nodes += 2 ** (self.depth)\n",
        "\n",
        "        return noise_sum\n",
        "\n",
        "\n",
        "from typing import List\n",
        "\n",
        "# do\n",
        "import torch\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "\n",
        "class FTRLM(Optimizer):\n",
        "    \"\"\"\n",
        "    implements FTRL optimizer to be used with Opacus or alternative noise generation\n",
        "    mechanism with (optional) momentum\n",
        "\n",
        "    DP-FTRL (private) paper: https://arxiv.org/abs/2103.00039\n",
        "    FTRL (non-private) paper: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        params,  # TODO: add tree aggregator\n",
        "        model_param_sizes: List[torch.Size] = [(1,)],\n",
        "        lr: float = 0.0,\n",
        "        momentum: float = 0.0,\n",
        "        dampening: float = 0.0,\n",
        "        nesterov: bool = False,\n",
        "        noise_std: float = 0.0,\n",
        "        max_grad_norm: float = 100.0,\n",
        "        seed: int = 0,\n",
        "        efficient: bool = False,\n",
        "        is_PCA_enabled: bool = False,\n",
        "        PCA_variance: float = 0.95,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            params = parameters for torch optimizer\n",
        "            lr = learning rate\n",
        "            momentum = # TODO: add implementation\n",
        "            nesterov = # TODO: add implementation\n",
        "            dampening = # # TODO: add implementation\n",
        "            noise_std = amount of noise to be added to teh\n",
        "        \"\"\"\n",
        "\n",
        "        # sanity checks for parameters\n",
        "        if lr <= 0.0:\n",
        "            raise ValueError(\"Invalid Learning Rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid Momentum Parameter: {}\".format(momentum))\n",
        "        if isinstance(max_grad_norm, float) and max_grad_norm <= 0:\n",
        "            raise ValueError(\n",
        "                \"max_grad_norm = {} is not a valid value. Please provide a float > 0.\".format(\n",
        "                    max_grad_norm\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.seed = seed\n",
        "        self.noise_std = noise_std\n",
        "        self.efficient = efficient\n",
        "        self.is_PCA_enabled = is_PCA_enabled\n",
        "        self.PCA_variance = PCA_variance\n",
        "\n",
        "        self.tree_aggregator = TreeAggregation(\n",
        "            grad_sizes=model_param_sizes,\n",
        "            std=self.noise_std,\n",
        "            efficient=self.efficient,\n",
        "            seed=self.seed,\n",
        "        )\n",
        "\n",
        "        # TODO: add weight decay\n",
        "        # if not 0.0 < weight_decay:\n",
        "        #    raise ValueError(\"Invalid Weight Decay Parameter: {}\".format(weight_decay))\n",
        "\n",
        "        if nesterov and (momentum <= 0 or dampening != 0):\n",
        "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
        "\n",
        "        defaults = dict(\n",
        "            lr=lr, dampening=dampening, momentum=momentum, nesterov=nesterov\n",
        "        )\n",
        "\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    def clip_gradient(self, grad):  # TODO: add per-example clipping\n",
        "        # print('Sanity Check', grad.shape)\n",
        "        if torch.norm(grad) > self.max_grad_norm:\n",
        "            return (grad * self.max_grad_norm) / float(torch.norm(grad))\n",
        "        return grad\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"performs one step of the DP-FTRL algorithm.\n",
        "\n",
        "        Args:\n",
        "            closure (callable, optional): a closure that reevaluates\n",
        "            the model and returns the loss\n",
        "        \"\"\"\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "        gradients = []\n",
        "        noisy_gradients = self.tree_aggregator.add_to_tree_and_get_sum(\n",
        "            gradients, add_gradient=False\n",
        "        )\n",
        "        for group in self.param_groups:\n",
        "            momentum = group[\"momentum\"]\n",
        "            dampening = group[\"dampening\"]\n",
        "            nesterov = group[\"nesterov\"]\n",
        "            lr = group[\"lr\"]\n",
        "            for p, noise in zip(group[\"params\"], noisy_gradients):\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "\n",
        "                d_p = p.grad\n",
        "                d_p = self.clip_gradient(d_p)\n",
        "                param_state = self.state[p]\n",
        "                if len(param_state) == 0:\n",
        "                    param_state[\"grad_sum\"] = torch.zeros_like(d_p)\n",
        "                    param_state[\"momentum\"] = torch.zeros_like(p)\n",
        "                    param_state[\"initial_model\"] = torch.zeros_like(p)\n",
        "                    param_state[\"initial_model\"].add_(p)  # only add initial model\n",
        "\n",
        "                initial_model, grad_sum = (\n",
        "                    param_state[\"initial_model\"],\n",
        "                    param_state[\"grad_sum\"],\n",
        "                )\n",
        "\n",
        "                grad_sum.add_(d_p)\n",
        "                # PCA\n",
        "                # noisy_gradient = noisy_gradients[indx]\n",
        "                \"\"\"if self.is_PCA_enabled:\n",
        "                    pca = PCA(self.PCA_variance)\n",
        "                    dimension2 = 1\n",
        "                    gradient_shape = noisy_gradient.shape\n",
        "                    for i in range(len(noisy_gradient.shape)):\n",
        "                        if i != 0:\n",
        "                            dimension2 *= gradient_shape[i]\n",
        "                    noisy_gradient_2D = noisy_gradient.reshape((gradient_shape[0], dimension2))\n",
        "                    noisy_transformed_gradient = pca.fit_transform(noisy_gradient_2D)\"\"\"\n",
        "\n",
        "                param_state[\"momentum\"].mul_(momentum).add_(\n",
        "                    grad_sum + noise, alpha=(1 - dampening)\n",
        "                )\n",
        "\n",
        "                # Nesterov\n",
        "                if nesterov:\n",
        "                    delta_w = (grad_sum + noise).add(\n",
        "                        param_state[\"momentum\"], alpha=momentum\n",
        "                    )\n",
        "                else:\n",
        "                    delta_w = param_state[\"momentum\"]\n",
        "\n",
        "                # p = initial_model\n",
        "                with torch.no_grad():\n",
        "                    p.copy_(initial_model - delta_w * lr)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "# do\n",
        "import unittest\n",
        "from copy import deepcopy\n",
        "\n",
        "\n",
        "class FTRLMTest(unittest.TestCase):\n",
        "    # init\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        # add multiple tests\n",
        "\n",
        "    # utils\n",
        "    def change_lr(self, opt, new_lr):\n",
        "        for group in opt.param_groups:\n",
        "            group[\"lr\"] = new_lr\n",
        "\n",
        "    def get_lr(self, opt):\n",
        "        for group in opt.param_groups:\n",
        "            return group[\"lr\"]\n",
        "\n",
        "    def get_momentum(self, opt):\n",
        "        for group in opt.param_groups:\n",
        "            return group[\"momentum\"]\n",
        "\n",
        "    # tests\n",
        "    def test_lr_deterministic(self):\n",
        "        param = torch.tensor([0.0], requires_grad=True)\n",
        "        param.grad = torch.tensor([1.0])\n",
        "        self.optimizer = FTRLM([param], lr=1, noise_std=0.0, max_grad_norm=10.0)\n",
        "        # self.optimizer.set_param_groups([param], lr=1)\n",
        "\n",
        "        # output = 0 - 1.0 * 1.0 = -1.0\n",
        "        self.assertAlmostEqual(self.get_lr(self.optimizer), 1.0)\n",
        "        self.optimizer.step()  # 1st step\n",
        "        print(param.item())\n",
        "        self.assertAlmostEqual(param.item(), -1.0, delta=1e-5)\n",
        "\n",
        "        # output = 0 - 1.5 * (1.0 + 1.0) = -3.0\n",
        "        self.change_lr(self.optimizer, 1.5)  #\n",
        "        self.assertAlmostEqual(self.get_lr(self.optimizer), 1.5)\n",
        "        self.optimizer.step()\n",
        "        print(param.item())\n",
        "        self.assertAlmostEqual(param.item(), -3.0, delta=1e-5)\n",
        "\n",
        "    def test_momentum(self):\n",
        "        param = torch.zeros((784, 10), requires_grad=True)\n",
        "        shapes = [param.shape]\n",
        "        # shapes = [p.shape for p in param]\n",
        "        print(shapes)\n",
        "        param.grad = torch.ones_like(param)\n",
        "        self.optimizer = FTRLM(\n",
        "            [param],\n",
        "            lr=0.1,\n",
        "            model_param_sizes=shapes,\n",
        "            noise_std=0.0,\n",
        "            momentum=0.9,\n",
        "            max_grad_norm=90.0,\n",
        "        )\n",
        "\n",
        "        self.assertAlmostEqual(self.get_lr(self.optimizer), 0.1)\n",
        "        self.assertAlmostEqual(self.get_momentum(self.optimizer), 0.9)\n",
        "        for epoch in range(2):\n",
        "            self.optimizer.step()\n",
        "        output = torch.flatten(param)\n",
        "        print(output)\n",
        "        self.assertTrue(\n",
        "            torch.allclose(\n",
        "                output,\n",
        "                torch.Tensor(\n",
        "                    [-0.29 * np.ones_like(p.detach().numpy()) for p in output]\n",
        "                ),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def test_ftrl_similar_sgd(self):  # without nosie should perform simlarly to SGD\n",
        "        param_optimizer = torch.normal(\n",
        "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
        "        )\n",
        "        # param_optimizer = torch.zeros((784, 10))\n",
        "        param_optimizer.grad = torch.normal(\n",
        "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
        "        )\n",
        "        # print('init grad', param_optimizer.grad)\n",
        "        shapes = [param_optimizer.shape]\n",
        "        param_sgd = deepcopy(param_optimizer)\n",
        "        param_sgd.grad = deepcopy(param_optimizer.grad)\n",
        "        self.optimizer = FTRLM(\n",
        "            [param_optimizer],\n",
        "            model_param_sizes=shapes,\n",
        "            lr=0.01,\n",
        "            noise_std=0.0,\n",
        "            max_grad_norm=500.0,\n",
        "        )\n",
        "        self.sgd = torch.optim.SGD([param_sgd], lr=0.01)\n",
        "\n",
        "        for epoch in range(3):\n",
        "            self.optimizer.step()\n",
        "            self.sgd.step()\n",
        "        print((param_optimizer), (param_sgd))\n",
        "        self.assertTrue(\n",
        "            torch.allclose(torch.flatten(param_optimizer), torch.flatten(param_sgd))\n",
        "        )\n",
        "\n",
        "    def test_ftrl_similar_sgd_momentum(\n",
        "        self,\n",
        "    ):  # without nosie should perform simlarly to SGD\n",
        "        param_optimizer = torch.normal(\n",
        "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
        "        )\n",
        "        # param_optimizer = torch.zeros((784, 10))\n",
        "        param_optimizer.grad = torch.normal(\n",
        "            mean=0.0, std=1.0, size=(784, 10), requires_grad=True\n",
        "        )\n",
        "        # print('init grad', param_optimizer.grad)\n",
        "        shapes = [param_optimizer.shape]\n",
        "        param_sgd = deepcopy(param_optimizer)\n",
        "        param_sgd.grad = deepcopy(param_optimizer.grad)\n",
        "        self.optimizer = FTRLM(\n",
        "            [param_optimizer],\n",
        "            model_param_sizes=shapes,\n",
        "            lr=0.01,\n",
        "            noise_std=0.0,\n",
        "            max_grad_norm=500.0,\n",
        "            momentum=0.9,\n",
        "        )\n",
        "        self.sgd = torch.optim.SGD([param_sgd], lr=0.01, momentum=0.9)\n",
        "\n",
        "        for epoch in range(3):\n",
        "            self.optimizer.step()\n",
        "            self.sgd.step()\n",
        "        print((param_optimizer), (param_sgd))\n",
        "        self.assertTrue(\n",
        "            torch.allclose(torch.flatten(param_optimizer), torch.flatten(param_sgd))\n",
        "        )\n",
        "\n",
        "\n",
        "test = FTRLMTest()\n",
        "test.test_lr_deterministic()  # DONE\n",
        "test.test_momentum()\n",
        "test.test_ftrl_similar_sgd()\n",
        "test.test_ftrl_similar_sgd_momentum()\n",
        "\n",
        "####DATALOADERS####\n",
        "\n",
        "####NET####\n",
        "\n",
        "\n",
        "# do\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "considered-fancy",
      "metadata": {
        "id": "considered-fancy"
      },
      "outputs": [],
      "source": [
        "#define the network in pytorch (with Z)\n",
        "class NeuralNetworkZ(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, lr_w, lr_z, batch_size):\n",
        "        super(NeuralNetworkZ, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim, hidden_dim[0])\n",
        "        self.lin2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
        "        self.z2 = nn.Parameter(torch.zeros(batch_size, hidden_dim[1]), requires_grad=True)\n",
        "        self.lin3 = nn.Linear(hidden_dim[1], hidden_dim[2])\n",
        "\n",
        "        self.apply(self.init_weights_biases)\n",
        "        self.apply(self.init_z)\n",
        "\n",
        "        self.optimizer_w = torch.optim.Adam(self.get_w(), lr=lr_w)\n",
        "        self.optimizer_z = torch.optim.Adam(self.get_z(), lr=lr_z)\n",
        "\n",
        "        self.criterion_z = nn.MSELoss()\n",
        "\n",
        "    def init_weights_biases(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.xavier_normal_(m.weight, gain=1.0) #modify with custom function later\n",
        "            nn.init.normal_(m.bias, mean=0.0, std=1.0)\n",
        "\n",
        "    def get_w(self):\n",
        "        return [self.input_layer.weight, self.lin2.weight, self.lin3.weight]\n",
        "\n",
        "    def init_z(self, m):\n",
        "        list_z = self.get_z()\n",
        "        for z in list_z:\n",
        "            z = nn.init.xavier_normal_(z, gain=1.0)\n",
        "\n",
        "    def get_z(self):\n",
        "        return [self.z2]\n",
        "\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        if training:\n",
        "            x = self.input_layer(x)\n",
        "            x = nn.Tanh()(x)\n",
        "            z2_tar = nn.Tanh()(self.lin2(x))\n",
        "            logits = self.lin3(self.z2)\n",
        "            return logits, z2_tar\n",
        "\n",
        "        else:\n",
        "            x = self.input_layer(x)\n",
        "            x = nn.Tanh()(x)\n",
        "            x = self.lin2(x)\n",
        "            x = nn.Tanh()(x)\n",
        "            x = self.lin3(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "    def z_loss(self, x):\n",
        "        logits, z2_tar = self.forward(x)\n",
        "        loss_z2 = self.criterion_z(self.z2, z2_tar)\n",
        "\n",
        "        return logits, loss_z2\n",
        "\n",
        "    def compute_ce_loss(self, logits, targets):\n",
        "        ce_loss = nn.CrossEntropyLoss()\n",
        "        final_loss = ce_loss(logits, targets)\n",
        "        return final_loss\n",
        "\n",
        "\n",
        "    def loss(self, x, targets, requires_z=True):\n",
        "        if requires_z:\n",
        "            logits, loss_z2 = self.z_loss(x)\n",
        "            final_loss = self.compute_ce_loss(logits, targets)\n",
        "            return final_loss, loss_z2\n",
        "\n",
        "        else:\n",
        "            logits = self.forward(x, requires_z=False)\n",
        "            final_loss = self.compute_ce_loss(logits, targets)\n",
        "            return final_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "gentle-colony",
      "metadata": {
        "id": "gentle-colony",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5eab73-d801-4b51-9a31-f81b0c7780f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_train/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 35630480.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_train/MNIST/raw/train-images-idx3-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_train/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1306834.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_train/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_train/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9521902.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_train/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_train/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5688423.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_train/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_train/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_test/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 35049554.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_test/MNIST/raw/train-images-idx3-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_test/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1135228.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_test/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_test/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 8104202.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_test/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_test/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4088973.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting mnist_test/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_test/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "input_dim = 784\n",
        "\n",
        "#Parameters setup\n",
        "classes = 10\n",
        "lr_w = 0.001\n",
        "lr_z = 0.1\n",
        "batch_size = 100\n",
        "hidden_dim = [100, 100, 10]\n",
        "epochs = 3\n",
        "\n",
        "#Get MNIST data and define transforms\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "trainset = datasets.MNIST('mnist_train', download=True, train=True, transform=transform)\n",
        "testset = datasets.MNIST('mnist_test', download=True, train=False, transform=transform)\n",
        "\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = NeuralNetworkZ(input_dim, hidden_dim, lr_w, lr_z, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4hr7Au7kgOBu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hr7Au7kgOBu",
        "outputId": "42764da1-c055-4e21-959a-9ce51c1cb89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#############params###############\n",
            "#############params###############\n"
          ]
        }
      ],
      "source": [
        "bs = 1000\n",
        "noise_multiplier = 7.0\n",
        "print(\"#############params###############\")\n",
        "param_list = []\n",
        "param_shapes = [p.shape for p in model.parameters()]\n",
        "for p in model.parameters():\n",
        "    param_list.append(p)\n",
        "print(\"#############params###############\")\n",
        "\n",
        "\n",
        "optimizer_FTRLM = FTRLM( # noqa\n",
        "        params=param_list,\n",
        "        model_param_sizes=param_shapes,\n",
        "        lr=0.01,\n",
        "        momentum=0.9,\n",
        "        nesterov=True,\n",
        "        noise_std=(noise_multiplier / bs),\n",
        "        max_grad_norm=1.0,\n",
        "        seed=0,\n",
        "        efficient=True,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "I6blhqCdhWca",
      "metadata": {
        "id": "I6blhqCdhWca"
      },
      "outputs": [],
      "source": [
        "model.optimizer_w = optimizer_FTRLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "chronic-tiffany",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "chronic-tiffany",
        "outputId": "ef5cd576-b6f7-466b-b926-e757f8ecaf8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step[100/600], Loss: 0.23467964, Loss Z2: 0.0122718383, CE Loss: 0.1119612604\n",
            "Accuracy of the network on the 10000 test images: 81.33 %\n",
            "Epoch [1/3], Step[200/600], Loss: 0.10287992, Loss Z2: 0.0049324203, CE Loss: 0.0535557158\n",
            "Accuracy of the network on the 10000 test images: 85.8 %\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "device = 'cpu'\n",
        "n_total_steps = len(trainloader)\n",
        "\n",
        "beta = 10\n",
        "z_iter = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    #reinit Z here every epoch\n",
        "\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        # Forward pass\n",
        "        #outputs = model(images)\n",
        "\n",
        "\n",
        "        for e in range(z_iter):\n",
        "            #progressively increase beta/gamma\n",
        "            L, loss_z2 = model.loss(images, labels)\n",
        "            loss_z2_temp = beta*loss_z2\n",
        "\n",
        "            total_loss_temp = L + loss_z2_temp\n",
        "\n",
        "            model.optimizer_z.zero_grad()\n",
        "            total_loss_temp.backward()\n",
        "            #loss_z2_temp.backward(retain_graph=True)\n",
        "\n",
        "            model.optimizer_z.step()\n",
        "            #scheduler.step(total_loss_temp) #if scheduler is used\n",
        "\n",
        "        loss, loss_z2  = model.loss(images, labels)\n",
        "        total_loss = loss + beta*loss_z2\n",
        "        # Backward and optimize\n",
        "        model.optimizer_w.zero_grad()\n",
        "        #loss.backward()\n",
        "        total_loss.backward()\n",
        "        model.optimizer_w.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{epochs}], Step[{i+1}/{n_total_steps}], Loss: {total_loss.item():.8f}, Loss Z2: {loss_z2.item():.10f}, CE Loss: {loss.item():.10f}')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                n_correct = 0\n",
        "                n_samples = 0\n",
        "                for images, labels in testloader:\n",
        "                    images = images.reshape(-1, 28*28).to(device)\n",
        "                    labels = labels.to(device)\n",
        "                    outputs = model(images, training=False)\n",
        "                    # max returns (value ,index)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    n_samples += labels.size(0)\n",
        "                    n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            acc = 100.0 * n_correct / n_samples\n",
        "            print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bridal-american",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bridal-american"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lined-vanilla",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lined-vanilla"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "right-greenhouse",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "right-greenhouse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interim-wilson",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "interim-wilson",
        "outputId": "5bef4b03-79fb-4ca9-85db-210e5f3a3f19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 92.89 %\n"
          ]
        }
      ],
      "source": [
        "#testing loop\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images, training=False)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "acc = 100.0 * n_correct / n_samples\n",
        "print(f'Accuracy of the network on the 10000 test images: {acc} %')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sitting-sacrifice",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sitting-sacrifice"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "particular-foster",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "particular-foster"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "enhanced-promise"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
